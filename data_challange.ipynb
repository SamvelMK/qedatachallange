{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing English News Articles That Mention \"Corona Virus\" Or \"Coronavirus\" Or \"Covid\" (By Webhose.Io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic description of the data: Source: https://webhose.io/free-datasets/news-articles-that-mention-corona-virus/\n",
    "* No. of Documents: 2,711,460\n",
    "* File Size (zipped): 13.7GB\n",
    "* Crawled Date: Dec, 2019 - Mar, 2020\n",
    "* Format Available: JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The design of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#     \"organizations\": [],\n",
    "#     \"uuid\": \"2b50b3f00e04fc17912154a7b88f3359db2b1ae8\",\n",
    "#     \"thread\": {\n",
    "#         \"social\": {\n",
    "#             \"gplus\": {\n",
    "#                 \"shares\": 0\n",
    "#             },\n",
    "#             \"pinterest\": {\n",
    "#                 \"shares\": 1\n",
    "#             },\n",
    "#             \"vk\": {\n",
    "#                 \"shares\": 0\n",
    "#             },\n",
    "#             \"linkedin\": {\n",
    "#                 \"shares\": 0\n",
    "#             },\n",
    "#             \"facebook\": {\n",
    "#                 \"likes\": 19,\n",
    "#                 \"shares\": 63,\n",
    "#                 \"comments\": 7\n",
    "#             },\n",
    "#             \"stumbledupon\": {\n",
    "#                 \"shares\": 0\n",
    "#             }\n",
    "#         },\n",
    "#         \"site_full\": \"www.foxnews.com\",\n",
    "#         \"main_image\": \"https://cf-images.us-east-1.prod.boltdns.net/v1/static/694940094001/abd745e2-d10e-43e1-b3b0-9acc892c7930/5df4c28e-39f8-464c-be01-944eb0ae53a6/1280x720/match/image.jpg\",\n",
    "#         \"site_section\": \"http://feeds.foxnews.com/foxnews/latest\",\n",
    "#         \"section_title\": \"FOX News\",\n",
    "#         \"url\": \"https://www.foxnews.com/media/dr-siegel-on-coronavirus-i-think-is-a-whopping-amount-of-cases-undiagnosed\",\n",
    "#         \"country\": \"US\",\n",
    "#         \"domain_rank\": 185,\n",
    "#         \"title\": \"Dr. Marc Siegel on coronavirus: 'I think it is a whopping amount of cases undiagnosed'\",\n",
    "#         \"performance_score\": 0,\n",
    "#         \"site\": \"foxnews.com\",\n",
    "#         \"participants_count\": 1,\n",
    "#         \"title_full\": \"\",\n",
    "#         \"spam_score\": 0.0,\n",
    "#         \"site_type\": \"news\",\n",
    "#         \"published\": \"2020-03-14T04:20:00.000+02:00\",\n",
    "#         \"replies_count\": 0,\n",
    "#         \"uuid\": \"2b50b3f00e04fc17912154a7b88f3359db2b1ae8\"\n",
    "#     },\n",
    "#     \"author\": \"Victor Garcia\",\n",
    "#     \"url\": \"https://www.foxnews.com/media/dr-siegel-on-coronavirus-i-think-is-a-whopping-amount-of-cases-undiagnosed\",\n",
    "#     \"ord_in_thread\": 0,\n",
    "#     \"title\": \"Dr. Marc Siegel on coronavirus: 'I think it is a whopping amount of cases undiagnosed'\",\n",
    "#     \"locations\": [],\n",
    "#     \"entities\": {\n",
    "#         \"persons\": [{\n",
    "#             \"name\": \"marc siegel\",\n",
    "#             \"sentiment\": \"negative\"\n",
    "#         }, {\n",
    "#             \"name\": \"siegel\",\n",
    "#             \"sentiment\": \"none\"\n",
    "#         }, {\n",
    "#             \"name\": \"tucker carlson\",\n",
    "#             \"sentiment\": \"none\"\n",
    "#         }, {\n",
    "#             \"name\": \"trump\",\n",
    "#             \"sentiment\": \"none\"\n",
    "#         }, {\n",
    "#             \"name\": \"trump\",\n",
    "#             \"sentiment\": \"none\"\n",
    "#         }],\n",
    "#         \"locations\": [{\n",
    "#             \"name\": \"us\",\n",
    "#             \"sentiment\": \"none\"\n",
    "#         }],\n",
    "#         \"organizations\": [{\n",
    "#             \"name\": \"fox news\",\n",
    "#             \"sentiment\": \"negative\"\n",
    "#         }]\n",
    "#     },\n",
    "#     \"highlightText\": \"\",\n",
    "#     \"language\": \"english\",\n",
    "#     \"persons\": [],\n",
    "#     \"text\": \"US doctors report inability to get tests for coronavirus patients Reaction from Fox News medical correspondent Dr. Marc Siegel. Dr. Marc Siegel appeared on \\\" Tucker Carlson Tonight \\\" on Friday where he gave his assessment of the coronavirus pandemic in the aftermath of President Trump declaring a national emergency. \\\"There's many thousands of cases that have not been diagnosed, possibly because they're mild, but it's not too late to test because we don't have another system we can work with,\\\" Siegel said.\\nTRUMP DECLARES NATIONAL EMERGENCY OVER CORONAVIRUS, ENLISTS PRIVATE SECTOR\\nSiegel also spoke about the problems hospitals and labs are having, fearing they are exposing their workers to the virus.\\n\\\"Doctors are being told you don't see these patients. Well, we don't know what to do with them then. And the only thing we have is a test, except you can't actually do the test because the lab, and I just found out this today... they're not going to do the tests,\\\" Siegel said. \\\"Tucker, even if they have the equipment, they don't want to put their lab technicians, in my opinion, in the line of fire and be subjected to possible coronavirus.\\\"\\nThe solution, according to the Fox News medical contributor, is to do what South Korea and facilities in Nebraska are doing -- drive-thru testing facilities.\\n\\\"You have to have people dressed up in personal protective equipment like we showed in Nebraska. They have to be doing [it] very carefully. And it's got to be done on a high volume basis anyway,\\\" Siegel said. \\\"It can't be contained anymore. But I'll tell you why I want it done.\\\"\\nCLICK HERE TO GET THE FOX NEWS APP\\nSiegel said although the virus has already spread, testing is vital to \\\"reassure people who don't have it\\\" and \\\"decrease the panic.\\\"\\n\\\"We have to know who has this so we can protect the people most at risk, even if it's sustained throughout all communities,\\\" Siegel said. \\\"I think it is a whopping amount of cases undiagnosed. We still need to know who has it.\\\" Get all the stories you need-to-know from the most powerful name in news delivered first thing every morning to your inbox Arrives Weekdays\",\n",
    "#     \"external_links\": [\"https://www.google.com/url\", \"https://google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=12&cad=rja&uact=8&ved=2ahUKEwjgyImt7pjoAhVKY6wKHQEiDKgQFjALegQICxAB&url=https%3A%2F%2Fwww.foxnews.com%2Fperson%2Fs%2Fmarc-siegel&usg=AOvVaw15JpfnAbH1BvHtx1er6nsI\", \"https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=12&cad=rja&uact=8&ved=2ahUKEwjgyImt7pjoAhVKY6wKHQEiDKgQFjALegQICxAB&url=https%3A%2F%2Fwww.foxnews.com%2Fperson%2Fs%2Fmarc-siegel&usg=AOvVaw15JpfnAbH1BvHtx1er6nsI\"],\n",
    "#     \"published\": \"2020-03-14T04:20:00.000+02:00\",\n",
    "#     \"crawled\": \"2020-03-14T04:31:41.175+02:00\",\n",
    "#     \"highlightTitle\": \"\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Steps\n",
    "1. Create an empty csv file with the relevant columns.\n",
    "2. Read each file in the data directory in chunks.\n",
    "3. Apply the data processing function\n",
    "4. Append each processed chunk to the csv datafile created in step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None # ignore the df chained assignment warnings for now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Social sites\n",
    "social = ['gplus', 'pinterest', 'vk', 'linkedin', 'stumbledupon']\n",
    "\n",
    "# This is for extracting relevant columns\n",
    "subset = ['title',  'country', 'external_links', 'url', 'text',\n",
    "         'gplus', 'pinterest', 'vk', 'linkedin', 'stumbledupon', 'likes', 'shares', 'comments', 'crawled', 'published'] # Comments, likes and shares are of facebook\n",
    "\n",
    "# This is for re-labeling fb related columns\n",
    "rename = {'shares' : 'fb',\n",
    "           'likes' : 'fb_likes', \n",
    "            'comments' : 'fb_comments'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis\n",
    "def sentiment(text):\n",
    "    ''' The function takes a text and returns a tuple with the polarity and subjectivity scores. '''\n",
    "    bolb = TextBlob(str(text))\n",
    "    return bolb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(data, social, subset, rename):\n",
    "    \n",
    "    \"\"\"\n",
    "    Data processing function. The function includes the following steps:\n",
    "                                1) takes a dataframe, \n",
    "                                2) extends columns that include nested dictionaries (e.g., thread),\n",
    "                                3) drops any duplicate columns,\n",
    "                                4) strip the linespace chars from title and text\n",
    "                                5) adds columns with the calculated sentiment scores (polarity and subjectivty) for title and text,\n",
    "                                6) drops the text (not to occupy the disk),\n",
    "                                7) return the processed dataframe.\n",
    "                                \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: dataframe,\n",
    "    social: list,\n",
    "    subset: list,\n",
    "    rename: dict,\n",
    "    \n",
    "    Return:\n",
    "    ------------\n",
    "    y : dataframe,\n",
    "    \n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    df = pd.concat([df.drop(['thread'], axis=1), df['thread'].apply(pd.Series)], axis=1)\n",
    "    df = pd.concat([df.drop(['social'], axis=1), df['social'].apply(pd.Series)], axis=1)\n",
    "    df = pd.concat([df.drop(['facebook'], axis=1), df['facebook'].apply(pd.Series)], axis=1)\n",
    "    \n",
    "    \n",
    "    # Get the values from each social media shares.\n",
    "    for i in social:\n",
    "        df[i] = df[i].apply(lambda x: x['shares'])\n",
    "    \n",
    "    # Remove any duplicate columns because of the thread fields.\n",
    "    df = df.loc[:,~df.columns.duplicated()] \n",
    "\n",
    "    # Subset data\n",
    "    sub = df[[i for i in subset]]\n",
    "\n",
    "    # Rename the facebook related columns.\n",
    "    sub.rename(columns = rename, inplace = True)\n",
    "    \n",
    "    # Strip the text of the linespace chars.\n",
    "    sub['title'] = sub['title'].apply(lambda x: x.replace('\\n', ' '))\n",
    "    sub['text'] = sub['text'].apply(lambda x: x.replace('\\n', ' '))\n",
    "    \n",
    "    # add sentiment scores (polarity, subjectivity) for title and text.\n",
    "    sub['title_pol'] = sub['title'].apply(lambda x: sentiment(x).polarity) \n",
    "    sub['title_sub'] = sub['title'].apply(lambda x: sentiment(x).subjectivity)\n",
    "    \n",
    "    sub['text_pol'] = sub['text'].apply(lambda x: sentiment(x).polarity) \n",
    "    sub['text_sub'] = sub['text'].apply(lambda x: sentiment(x).subjectivity)\n",
    "    \n",
    "    # drop the column with the text.\n",
    "    sub.drop(['text'], axis=1, inplace = True)\n",
    "    \n",
    "    return sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1) Create an empty csv file with the relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [ 'title', 'country', 'external_links', 'url', \n",
    "            'gplus', 'pinterest', 'vk', 'linkedin', 'stumbledupon', 'fb_likes', 'fb', 'fb_comments', 'crawled', 'published', \n",
    "           'title_pol', 'title_sub', 'text_pol', 'text_sub']\n",
    "empty = pd.DataFrame(columns = columns)\n",
    "empty.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps 2-4) Run the data Processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n",
      "**\n",
      "***\n",
      "****\n",
      "*****\n",
      "******\n",
      "*******\n",
      "********\n",
      "*********\n",
      "**********\n",
      "***********\n",
      "************\n",
      "*************\n",
      "**************\n",
      "***************\n",
      "****************\n",
      "*****************\n",
      "******************\n",
      "*******************\n",
      "********************\n",
      "*********************\n",
      "**********************\n",
      "***********************\n",
      "************************\n",
      "*************************\n",
      "**************************\n",
      "***************************\n",
      "****************************\n",
      "*****************************\n",
      "******************************\n",
      "*******************************\n",
      "Time taken = 18913.622400283813 seconds\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "starttime = time.time()\n",
    "\n",
    "for i in os.listdir('data/'):    \n",
    "    data = pd.read_json(f'data/{i}', lines=True, dtype = True, chunksize=100)\n",
    "    for i in data:\n",
    "        res = process(i, social, subset, rename)\n",
    "        res.to_csv('data.csv', mode='a', header=False, index=True)\n",
    "    counter +=1\n",
    "    print('*'*counter)\n",
    "print('Time taken = {} seconds'.format(time.time() - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
